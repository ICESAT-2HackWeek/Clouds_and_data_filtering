{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "data_dir='ATL06/Byrd_glacier_rel001/'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "# make sure we're dealing with the most recent version of any code we're using\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clouds over land ice: What problems they cause, and what to do about it.\n",
    "This tutorial covers some of the reasons you might see weird results over ice when clouds start to blot out the surface signals.  The learning objectives I'd like to get to are:\n",
    "- Understanding how clouds affect laser-altimetry signals\n",
    "- Recognizing how these effects are manifest in the ATL06 product\n",
    "- Gaining familiarity with the ATL06 parameters that can identify cloudy returns\n",
    "This part of the tutorial will focus on clouds that cause gross errors in surface-height estimates.  \n",
    "\n",
    "Along the way, we'll:\n",
    "\n",
    "- develop a simple function for reading ICESat-2 data from hdf-5 files\n",
    "\n",
    "If time allows, I'll also present on subtler effects caused by forward scattering of laser light by thin clouds, which can lead to smaller elevation biases.  The objectives of this part of the tutorial will be:\n",
    "- Understanding how forward scattering of laser light by clouds can introduce biases in surface-height estimates\n",
    "- Tools for identifying returns affected by forward scattering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary matters:  \n",
    "I've pulled the ICESat-2 data for the Byrd Glacier catchment, and it's stored on S3, so you can grab it with these commands:\n",
    "\n",
    "    aws s3 cp s3://pangeo-data-upload-oregon/icesat2/Clouds_and_filtering_tutorial/ATL06 /home/jovyan/ATL06 --recursive\n",
    "    aws s3 cp s3://pangeo-data-upload-oregon/icesat2/Clouds_and_filtering_tutorial/ATL03 /home/jovyan/ATL03 --recursive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Python concepts that we'll be using\n",
    "We are going to be using a couple of python features to hold and manipulate data.  Many of you already know how to do this, but just in case, here's a review.\n",
    "\n",
    "### 0.1 lists\n",
    "Python stores groups of data in lists.  You can make an empty list, or a list that already contains other things using square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_list=[]\n",
    "not_empty_list=[\"some\", \"things\", \"in\", \"a\", \"list\"]\n",
    "print(\"Empty list:\")\n",
    "print(empty_list)\n",
    "print(\"not empty list:\")\n",
    "print(not_empty_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add more elements to your using a plus sign.  More or less equivalent is to use the \".append\" method of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_empty_list1 = not_empty_list + [\"And yet more\"]\n",
    "empty_list.append(\"not so empty\")\n",
    "print(not_empty_list1)\n",
    "print(empty_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can acess the elements in a list by their index.  It's handy to know that negative indices access the list starting from the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the second element in not_empty_list1:\")\n",
    "print(not_empty_list1[1])\n",
    "print(\"the last element in not_empty_list1:\")\n",
    "print(not_empty_list1[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Dictionaries\n",
    "Dictionaries are like lists, except that instead of being indexed by numbers, they are indexed by keys, and the keys can be just about anything (there are a few things that don't work).  \n",
    "\n",
    "Dictionaries are defined by curly braces, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dictionary:\n",
    "a={}\n",
    "# dictionary with something already in it:\n",
    "b={\"ICESat-2\":\"Ice, Cloud and land Elevation Satellite\", \n",
    "   \"TL/DR\":\"everything you want in a tutorial.\", \n",
    "   3:\"the first number after 2\"}\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extract an entry from a dictionary by putting it in square brackets after the dictionary name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"b[3] is:\")\n",
    "print(b[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets us use dictionaries as data containers, where we can keep the data fields organized by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'x':np.array([1, 2, 3, 4, 5]), 'y':np.array([5, 4, 3, 4, 5])}\n",
    "plt.plot(data['x'], data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one more important thing:\n",
    "       <img \"https://media1.tenor.com/images/cc7dab6100dcfd611d435fa8bbf2e896/tenor.gif?itemid=4997651\">\n",
    "_url credit: Matt Siegfried_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Introduction: Signals and signal finding.\n",
    "\n",
    "<img src=\"images/Four_photon_paths_sm.jpg\"  width=600 height=600>\n",
    "\n",
    "The photons that we see in ATLAS data can come to the receiver in a few different ways.  \n",
    "--(A, B): The majority of photons come from the sun.   These may reflect off clouds or off the land surface, and they arrive with a uniform random distribution. ATLAS does a good job of filtering these out, but some make it through into the data.  \n",
    "--C: A few are ATLAS laser photons reflected off clouds.  These can be loosely clustered in space, or widely spread out like background photons.  \n",
    "--D, E: Some are ATLAS photons that reflect off the surface.  These are the ones that most of us are interested in.\n",
    "\n",
    "Our challenge is to collect and analyze D and E, without getting confused by A, B, or C.  Part of the challenge is that there are many photons from ATLAS that end up in category F: photons that are scattered away by clouds, and never seen again.\n",
    "\n",
    "There are three stages that determine whether a surface in a higher-level product like ATL06.\n",
    "1. The ATLAS on-board algorithm may or may not decide to telemeter a photon.  ATLAS doesn't have a lot of computing power, and it can't transmit every photon to the surface.  It uses a coarse DEM and some simple rules to decide whether to transmit large blocks of data to the surface.  If it can't find an obvious group of photons from the surface, it usually transmits the strongest block of photons it finds.  This often works well.\n",
    "2. ATL03 uses a surface-type-dependent algorithm to search for spatially-extensive clusters of photons, and marks them in the _signal_confidence_ flags.  This mostly works very well.\n",
    "3. ATL06 uses the ATL03 signal confidence flags, when they are available, to find the surface.  If this fails, ATL06 uses a backup signal finder to try to find the surface that ATL03 missed.  \n",
    "\n",
    "If we look at an ATL03 we will see all of the photons that ATLAS telemetered to the ground, and we will see the results of ATL03 ground finding.  If we look at an ATL06, we will see the heights of segments for which ATL06 decided that there were enough photons that it might have detected the ground. We will look at some data to try to understand how these processes worked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 ATL06 signal processing\n",
    "\n",
    "<img src=\"images/ATL06_segment_model.png\"  width=600 height=600>\n",
    "\n",
    "Recall that ATL06 gives us surface heights based on the heights of collections of photons in a 40-m (horizontal) by w_surface_window (vertical) window.  It uses a variety of techniques to shrink the window to the smallest size that contains the surface without clipping off any signal photons.  \n",
    "\n",
    "\n",
    "There's a general philosophy that went into the design of ATL06:\n",
    "1. Use the best available information to identiy the surface\n",
    "2. If there's a chance that we've found a weak surface return, report it\n",
    "3. Provide enough parameters that users can decide which returns are worth using, and which are not\n",
    "\n",
    "When there are no thick clouds between ICESat-2 and the surface, finding the surface return and reporting its height is straightforward: ATL03 provides a tight cluster of high-confidence photons, and ATL06 calculates a weighted average of their heights.\n",
    "\n",
    "Once clouds start to block some of the laser light, the number of photons that return to ATLAS from each return becomes progressively smaller.  Unfortunately, even if there are no laser photons to measure, during daylight there is no shortage of other photons to track.  ATLAS does a very good job of filtering out almost all of these photons, but on a sunny day, over a white surface, the measured background rate can be as high as 12 MHz.  Converting to dimensions that we'll be seeing this is:\n",
    "$$ \n",
    "    \\frac{1.2\\times10^7 photons}{second} \\times \\frac{1 second}{1.5\\times10^8 m} = \\frac{1 photon}{12.5 m}\n",
    "$$\n",
    "This doesn't sound like a lot, but over a 10-meter-high window that's 40 m long (typical for the kind of windows you might use to look for the surface if you didn't know where to find it) we can expect to find 45 photons.\n",
    "\n",
    "Unlike surface-return photons, background photons are uniformally distributed in height, and any clustering of these photons will be due to random chance.  When the signal quality is marginal, ATL03 may flag photons only as low- and medium- confidence for a particular segment, or may flag no photons at all.  If ATL03 hasn't told ATL06 which photons are the surface, the algorithm uses a backup signal-finding strategy that initializes surface finding using the strongest cluster of photons available.  It then attempts to converge its surface window on a tight cluster of photons.  This occasionally works, but if there really is no signal, the size of the window generally remains large, and we can evaluate the results based on the signal-to-noise ratio (SNR) of whatever ends up inside the window.  Only those segments with at least 10 photons, for which the probability of converging to an SNR equal to the observed SNR or better for random-noise inputs is less thatn 5% are reported.  This cuts down on false positives considerably.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A look at ATL03 over Byrd Glacier\n",
    "\n",
    "This part of the tutorial is possible thanks to a function provided by Tyler Sutterly \\[stand up, Tyler\\].\n",
    "\n",
    "To get an idea of how this works, we'll read in an ATL03 granule over Byrd Glacier in East Antarctica.  We'll use a reader that we import from an external Python file (readers/read_HDF5_ATL03.py, and a helper function, /readers/get_ATL03_x_atc.py), and plot the results using Matplotlib.  ATL03 is a complicated product, and reading it takes quite a few lines of code.  Please feel free to look at Tyler's code, and ask him questions about it.  We won't cover this in detail during the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readers.read_HDF5_ATL03 import read_HDF5_ATL03\n",
    "from readers.get_ATL03_x_atc import get_ATL03_x_atc\n",
    "\n",
    "# read the IS2 data with Tyler's ATL03 reader:\n",
    "ATL03_file=glob('/home/jovyan/ATL03/Byrd_Glacier_rel001/*.h5')\n",
    "IS2_atl03_mds, IS2_atl03_attrs, IS2_atl03_beams =read_HDF5_ATL03(ATL03_file[0])\n",
    "\n",
    "# add x_atc to the ATL03 data structure (this function adds to the LS2_ATL03_mds dictionary)\n",
    "get_ATL03_x_atc(IS2_atl03_mds, IS2_atl03_attrs, IS2_atl03_beams)\n",
    "\n",
    "#-- select the 1r beam from ATL03\n",
    "D3 = IS2_atl03_mds['gt1r']\n",
    "\n",
    "#-- create scatter plot of photon data (e.g., photon elevation vs x_atc)\n",
    "%matplotlib inline\n",
    "f1,ax = plt.subplots(num=1,figsize=(10,6))\n",
    "plt.plot(D3['heights']['x_atc'], D3['heights']['h_ph'],'k.',markersize=0.1)\n",
    "ax.set_xlabel('x_atc, m')\n",
    "ax.set_ylabel('h, m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to notice. First, the width of this plot is about 120 km, and its height(top to bottom) is about 2 km.  We're looking at large structures here.  Second, This is mostly a plot of background photon heights. The black shapes here represent the top and bottom of the ATLAS telemetry window, so it's showing what the simple electronics on board the satellite identified as potentially coming from the surface. You can see that the ATLAS signal finder has locked onto something on the left-hand side of the plot (up to around $2.904\\times 10^7 m$), then it loses track of the surface in the middle of the plot between 2.904 and $2.910\\times 10^7 m$ and reports its best guess, then maybe picks the surface up again around $2.910\\times 10^7 m$.  We can get a better idea of these by zooming in on a couple of regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick some subsets of the data.  These are logical arrays- what's marked as true in these arrays will get plotted, what's not will be ignored\n",
    "subset1=(D3['heights']['x_atc'] > 2.90190e7) & ((D3['heights']['x_atc'] < 2.902e7) )\n",
    "subset2=(D3['heights']['x_atc'] > 2.90590e7) & ((D3['heights']['x_atc'] < 2.906e7) )\n",
    "subset3=(D3['heights']['x_atc'] > 2.910e7) & ((D3['heights']['x_atc'] < 2.9101e7) )\n",
    "\n",
    "f1,ax = plt.subplots(1, 3,figsize=(10,6))\n",
    "ax[0].plot(D3['heights']['x_atc'][subset1], D3['heights']['h_ph'][subset1],'k.',markersize=0.5)\n",
    "ax[0].set_xlabel('x_atc, m')\n",
    "ax[0].set_ylabel('height, m')\n",
    "ax[0].set_title('likely signal')\n",
    "plt.axes(ax[1])\n",
    "ax[1].plot(D3['heights']['x_atc'][subset2], D3['heights']['h_ph'][subset2],'k.',markersize=0.5)\n",
    "ax[1].set_xlabel('x_atc, m')\n",
    "ax[1].set_ylabel('height, m')\n",
    "ax[1].set_title('likely noise')\n",
    "plt.axes(ax[2])\n",
    "ax[2].plot(D3['heights']['x_atc'][subset3], D3['heights']['h_ph'][subset3],'k.',markersize=0.5)\n",
    "ax[2].set_xlabel('x_atc, m')\n",
    "ax[2].set_ylabel('height, m')\n",
    "ax[2].set_title('Also likely noise')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our guess that the area on the left-hand side of the plot was signal was right, but the regions we picked from the middle and right-hand side of the plot only contain noise.  This is not a very good way of looking at data.  ATL03 has automatic signal finders that do a good job of identifying the surface.  Here's what they look like for this granule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1,ax = plt.subplots(num=1,figsize=(10,6))\n",
    "\n",
    "\n",
    "#-- check confidence level associated with each photon event\n",
    "#-- -1: Events not associated with a specific surface type\n",
    "#--  0: noise\n",
    "#--  1: buffer but algorithm classifies as background\n",
    "#--  2: low\n",
    "#--  3: medium\n",
    "#--  4: high\n",
    "#-- Signal classification confidence for land ice\n",
    "#-- 0=Land; 1=Ocean; 2=SeaIce; 3=LandIce; 4=InlandWater\n",
    "ice_sig_conf = D3['heights']['signal_conf_ph'][:,3]\n",
    "#-- background and buffer photons\n",
    "bg, = np.nonzero((ice_sig_conf == 0) | (ice_sig_conf == 1))\n",
    "lc, = np.nonzero(ice_sig_conf == 2)\n",
    "mc, = np.nonzero(ice_sig_conf == 3)\n",
    "hc, = np.nonzero(ice_sig_conf == 4)\n",
    "#-- Photon event delta time and elevation (WGS84)\n",
    "ax.plot(D3['heights']['x_atc'][bg],D3['heights']['h_ph'][bg],marker='.',\n",
    "    markersize=0.1,lw=0,color='gray',label='Background')\n",
    "ax.plot(D3['heights']['x_atc'][lc],D3['heights']['h_ph'][lc],marker='.',\n",
    "    markersize=0.25,lw=0,color='darkorange',label='Low Confidence')\n",
    "ax.plot(D3['heights']['x_atc'][mc],D3['heights']['h_ph'][mc],marker='.',\n",
    "    markersize=0.25,lw=0,color='mediumseagreen',label='Medium Confidence')\n",
    "ax.plot(D3['heights']['x_atc'][hc],D3['heights']['h_ph'][hc],marker='.',\n",
    "    markersize=0.25,lw=0,color='darkorchid',label='High Confidence')\n",
    "#-- set title and labels\n",
    "ax.set_xlabel('along-track distance, m')\n",
    "ax.set_ylabel('Elevation above WGS84 Ellipsoid [m]')\n",
    "ax.set_title(\"ATL03 with signal finding\")\n",
    "#-- create legend\n",
    "lgd = ax.legend(loc=3,frameon=False)\n",
    "lgd.get_frame().set_alpha(1.0)\n",
    "for line in lgd.get_lines():\n",
    "    line.set_linewidth(6)\n",
    "#-- adjust ticks\n",
    "ax.get_xaxis().set_tick_params(which='both',direction='in')\n",
    "ax.get_yaxis().set_tick_params(which='both',direction='in')\n",
    "#-- show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters of photons are great, but they don't tell you how high the surface is.  ATL06 is where we want to go for that.  Because the ATL06 product is simpler, we can write a fairly simple function that reads useful data into a dictionary.  The code is stored in readers/ATL06_to_dict.py, but we can also look at it in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "def ATL06_to_dict(filename, dataset_dict):\n",
    "    \"\"\"\n",
    "        Read selected datasets from an ATL06 file\n",
    "\n",
    "        Input arguments:\n",
    "            filename: ATl06 file to read\n",
    "            dataset_dict: A dictinary describing the fields to be read\n",
    "                    keys give the group names to be read, \n",
    "                    entries are lists of datasets within the groups\n",
    "        Output argument:\n",
    "            D6: dictionary containing ATL06 data.  Each dataset in \n",
    "                dataset_dict has its own entry in D6.  Each dataset \n",
    "                in D6 contains a list of numpy arrays containing the \n",
    "                data\n",
    "    \"\"\"\n",
    "    \n",
    "    D6=[]\n",
    "    pairs=[1, 2, 3]\n",
    "    beams=['l','r']\n",
    "    # open the HDF5 file\n",
    "    with h5py.File(filename) as h5f:\n",
    "        # loop over beam pairs\n",
    "        for pair in pairs:\n",
    "            # loop over beams\n",
    "            for beam_ind, beam in enumerate(beams):\n",
    "                # check if a beam exists, if not, skip it\n",
    "                if '/gt%d%s/land_ice_segments' % (pair, beam) not in h5f:\n",
    "                    continue\n",
    "                # loop over the groups in the dataset dictionary\n",
    "                temp={}\n",
    "                for group in dataset_dict.keys():\n",
    "                    for dataset in dataset_dict[group]:\n",
    "                        DS='/gt%d%s/%s/%s' % (pair, beam, group, dataset)\n",
    "                        # since a dataset may not exist in a file, we're going to try to read it, and if it doesn't work, we'll move on to the next:\n",
    "                        try:\n",
    "                            temp[dataset]=np.array(h5f[DS])\n",
    "                            # some parameters have a _FillValue attribute.  If it exists, use it to identify bad values, and set them to np.NaN\n",
    "                            if '_FillValue' in h5f[DS].attrs:\n",
    "                                fill_value=h5f[DS].attrs['_FillValue']\n",
    "                                bad=temp[dataset]==fill_value\n",
    "                                temp[dataset]=np.float64(temp[dataset])\n",
    "                                temp[dataset][bad]=np.NaN\n",
    "                        except KeyError as e:\n",
    "                            pass\n",
    "                if len(temp) > 0:\n",
    "                    # it's sometimes convenient to have the beam and the pair as part of the output data structure: This is how we put them there.\n",
    "                    temp['pair']=np.zeros_like(temp['h_li'])+pair\n",
    "                    temp['beam']=np.zeros_like(temp['h_li'])+beam_ind\n",
    "                    temp['filename']=filename\n",
    "                    D6.append(temp)\n",
    "    return D6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a list of dictionary objects, where each dictionary contains the variables in the ATL06 product.  To tell the reader which variables to read, we just provide a dictionary that tells it where to look in the file (which groups to read) and what variables to read from each group.  Were going to read:\n",
    "- /gtxx/land_ice_segments/h_li   : the land-ice height.\n",
    "- /gtxx/land_ice_segments/delta_time : the time of the segment.\n",
    "- /gtxx/land_ice_segments/ground_track/x_atc  : the along-track coordinate of the segment.\n",
    "\n",
    "We define the dictionary that tells the function what to read like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict={'land_ice_segments':['h_li', 'delta_time','longitude','latitude'], 'land_ice_segments/ground_track':['x_atc']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then we use the reader function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read ATL06 into a dictionary (the ATL06 file has the same name as the ATL03 file, except for the product name)\n",
    "ATL06_file=ATL03_file[0].replace('ATL03', 'ATL06')\n",
    "D6_list=ATL06_to_dict(ATL06_file, dataset_dict)\n",
    "\n",
    "# pick out gt1r:\n",
    "D6 = D6_list[1]\n",
    "\n",
    "# plot ATL03, with ATL06 on top:\n",
    "f1,ax = plt.subplots(num=1,figsize=(10,6))\n",
    "ax.plot(D3['heights']['x_atc'], D3['heights']['h_ph'],'k.',markersize=0.1, label='ATL03')\n",
    "ax.plot(D6['x_atc'], D6['h_li'],'r.', markersize=2, label='ATL06')\n",
    "lgd = ax.legend(loc=3,frameon=False)\n",
    "\n",
    "ax.set_xlabel('x_atc, m')\n",
    "ax.set_ylabel('h, m')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is that ATL06 found the surface in the same places that ATL03 did, but also reported heights in some of the noise-only areas. This is a consequence of attempting to find surfaces under marginal conditions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bad_segments=np.sum((D6['x_atc'] > 2.904e7) & (D6['x_atc'] < 2.912e7) & np.isfinite(D6['h_li']))\n",
    "num_possible_segments=(2.912e7 - 2.904e7)/20\n",
    "F_bad=num_bad_segments/num_possible_segments\n",
    "print(\"fraction of bad segments = %3.3f\" % F_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal-to-noise threshold on ATL06 is supposed to let through about 5% of noise-only segments.  If we assume that everything beyond $2.094\\times 10^7 m$ is noise, we find that ATL06 found a (bad) height about 4% of the time.  We'll work on cleaning these up in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Relating cloud effects to ATL06 data parameters\n",
    "\n",
    "Our problem comes when the ground return is not strong enough to trigger the signal finder, and we start to see triggers associated with:\n",
    "- Cloud tops\n",
    "- Random clusterings of background photons\n",
    "\n",
    "These should both be statistically distinct from surface returns because:\n",
    "- The returns are less intense than a high-quality surface return\n",
    "- The photons are more widely vertcally spread than those in surface returns\n",
    "- The surface window cannot converge on a small vertical window around the surface\n",
    "- Surface heights and slopes are not consistent between adjacent segments\n",
    "\n",
    "There are a few ATL06 parameters that help quantify these distinctions. \n",
    "In the /gtxx/land_ice_segments group:\n",
    "- h_li_sigma : the estimated error in the surface-height estimate\n",
    "In the /gtxx/land_ice_segments/fit_statistics groups:\n",
    "- n_fit_photons : The number of photons found in each segment\n",
    "- w_surface_window_final : The size of the converged surface window\n",
    "- h_rms_misft : The RMS misfit of photons in the surface window\n",
    "- h_robust_sprd : A percentile-based estimate of the spread of the photons, corrected for background\n",
    "- snr : the observed signal-to-noise ratio for the selected photons\n",
    "- snr_significance : The estimated probability that a random clustering of photons would produce the observed SNR\n",
    "- dh_fit_dx : the along-track segment slope\n",
    "And in /gtxx/land_ice_segments/geophysical:\n",
    "- r_eff : the effective reflectance of the surface\n",
    "\n",
    "There's one more parameter that puts a few of these ideas together, in /gtxx/land_ice_segments:\n",
    "- atl06_quality_summary : a combination of parameters (h_li_sigma, n_fit_photons/w_surface_window_final, and signal_selection_source).  Zero indicates a good segment, 1 indicates a possibly bad segment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Taking our picture of cloud effects to the ice sheet\n",
    "We're next going to look at data from Byrd Glacier, in Antarctica.  This is one of the big outlet glaciers dumping ice from the East Antarctic plateau into the Ross Ice Shelf.  Here's a shaded-relief map from the REMA.\n",
    "<img src=\"images/byrd_rema_sm.jpg\">\n",
    "The glacier flows west to east (the REMA image is south-end-up, so east is on the left).  The inland catchment of the glacier should have a smooth ice surface, but the glacier trunk and the area where it joins the ice shelf are heavily crevassed.\n",
    "\n",
    "The dataset on AWS contains a large number of granules that I found with a spatial search using an NSIDC data query.  We can use matplotlib's scatter() function to make a map of all the elevations from all of these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: From now on, we'll be working in matplotlib's widget mode, which lets us zoom in on our plots.  \n",
    "# This means that the figures won't be rendered in the notebook until you run them.\n",
    "# That means no more spoiler plots (for now...)\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "data_dir='/home/jovyan/ATL06/Byrd_Glacier_rel001/'\n",
    "D6=[]\n",
    "pairs=[1, 2, 3]\n",
    "beams=['l','r']\n",
    "\n",
    "files=glob(data_dir+'/*.h5')\n",
    "for file in files:\n",
    "    this_name=os.path.basename(file)\n",
    "    D6 += ATL06_to_dict(file, dataset_dict)\n",
    "print(\"read %d beam/pair combinations\" % (len(D6)))\n",
    "\n",
    "# now plot the results:\n",
    "plt.figure();\n",
    "for Di in D6:\n",
    "    plt.scatter(Di['longitude'], Di['latitude'], c=Di['h_li'], vmin=0, vmax=2000, linewidth=0)\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is opposite the orientation of the REMA map, but we can see the trunk of the glacier (dark blue, right-center) and we can see the East Antarctic plateau (bright yellow, at left).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few granules as plots of height against along-track distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict={'land_ice_segments':['latitude','longitude','h_li'], 'land_ice_segments/ground_track':['x_atc']}\n",
    "\n",
    "%matplotlib widget\n",
    "print(len(files))\n",
    "for file in files[52:55]:\n",
    "    this_D6=ATL06_to_dict(file, dataset_dict)\n",
    "    plt.figure()\n",
    "    plt.plot(this_D6[1]['x_atc'], this_D6[1]['h_li'],'.')\n",
    "    plt.title(this_D6[1]['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant exercise:\n",
    "\n",
    "Let's split up into groups for a few minutes, and have a look through these granules.  Try to find some examples with \n",
    "- Clouds\n",
    "- Really steep surface slopes\n",
    "- Crevasses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are some of the files that I picked out as interesting:\n",
    "\n",
    "example_file_indices=[3, 1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first example\n",
    "D6=ATL06_to_dict(files[example_file_indices[0]], dataset_dict)\n",
    "markers=['.','+']\n",
    "plt.figure()\n",
    "for ind, seg in enumerate(D6[0:2]):     \n",
    "    plt.plot(seg['x_atc'], seg['h_li'],marker=markers[ind], color='k', linestyle='')\n",
    "    plt.title(seg['filename'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our old friend, with good quality on the left, and bad quality on the right.  The weak beam of pair 1 is plotted with points, the strong beam is plotted with plusses- clearly having four times fewer photons in the weak beam doesn't give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D6=ATL06_to_dict(files[example_file_indices[1]], dataset_dict)\n",
    "markers=['.','+']\n",
    "plt.figure()\n",
    "for ind, seg in enumerate(D6[0:2]):     \n",
    "    plt.plot(seg['x_atc'], seg['h_li'],marker=markers[ind], color='k', linestyle='')\n",
    "    plt.title(seg['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is more interesting.  The quality is generally high, but there are a few places where the surface disappears and there are outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D6=ATL06_to_dict(files[example_file_indices[2]], dataset_dict)\n",
    "markers=['.','+']\n",
    "plt.figure()\n",
    "for ind, seg in enumerate(D6[0:2]):     \n",
    "    plt.plot(seg['x_atc'], seg['h_li'],marker=markers[ind], color='k', linestyle='')\n",
    "    plt.title(seg['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one worked well. ATL06 maintained a good lock on the surface across some very rugged terrain, with no significant false detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant exercise:\n",
    "Look at the Byrd Glacier profiles.  \n",
    "- Use the matplotlib scatter function to plot the parameters as a function of x_atc and h_li, and see which parameters correspond to good returns, which correspond to clouds.\n",
    "- See which returns are lost if we use the parameter as a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:  \n",
    "dataset_dict['/land_ice_segments/fit_statistics']=['h_rms_misfit']\n",
    "D6=ATL06_to_dict(files[example_file_indices[2]], dataset_dict)\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.scatter(D6[0]['x_atc'], D6[0]['h_li'], c=D6[0]['h_rms_misfit'], linewidth=0); plt.colorbar()\n",
    "plt.subplot(122)\n",
    "good=np.where(D6[0]['h_rms_misfit'] < .5)[0]\n",
    "plt.plot(D6[0]['x_atc'][good], D6[0]['h_li'][good],'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:  \n",
    "dataset_dict['/land_ice_segments/fit_statistics']=['h_rms_misfit']\n",
    "D6=ATL06_to_dict(files[54], dataset_dict)\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.scatter(D6[0]['x_atc'], D6[0]['h_li'], c=D6[0]['h_rms_misfit'], linewidth=0); plt.colorbar()\n",
    "plt.subplot(122)\n",
    "good=np.where(D6[0]['h_rms_misfit'] < .5)[0]\n",
    "plt.plot(D6[0]['x_atc'][good], D6[0]['h_li'][good],'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have picked out a set of filtering parameters you like, compare the results with the atl06_quality_summary. Do you have anything you like better?  Regenerate the plot from the top of this section where we mapped the heights for Byrd Glacier using plt.scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more way of flagging bad data that we can try, that can help clean up the remaining bad segments.  Recall that the the ATL06 data model fits both the height and the surface slope of segments: \n",
    "\n",
    "<img src=\"images/dh_segment_sm.jpg\"  width=500 height=450>\n",
    "\n",
    "Let's look at the segments including their slopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_slope_plot(D6):\n",
    "    \"\"\"\n",
    "    Plot a sloping line for each ATL06 segment\n",
    "    \"\"\"\n",
    "    #define the heights of the segment endpoints.  Leave a row of NaNs so that the endpoints don't get joined\n",
    "    h_ep=np.zeros([3, D6['h_li'].size])+np.NaN\n",
    "    h_ep[0, :]=D6['h_li']-D6['dh_fit_dx']*20\n",
    "    h_ep[1, :]=D6['h_li']+D6['dh_fit_dx']*20\n",
    "    # define the x coordinates of the segment endpoints\n",
    "    x_ep=np.zeros([3,D6['h_li'].size])+np.NaN\n",
    "    x_ep[0, :]=D6['x_atc']-20\n",
    "    x_ep[1, :]=D6['x_atc']+20\n",
    "\n",
    "    plt.plot(x_ep.T.ravel(), h_ep.T.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to try this, we'll need to add dh_fit_dx to the data dictionary:\n",
    "dataset_dict['land_ice_segments/fit_statistics']=['dh_fit_dx']\n",
    "\n",
    "D6=ATL06_to_dict(files[example_file_indices[0]], dataset_dict)\n",
    "markers=['.','+']\n",
    "plt.figure()\n",
    "for ind, seg in enumerate(D6[0:2]):     \n",
    "    seg_slope_plot(seg)\n",
    "    plt.title(seg['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we zoom in we can see that the \"smooth\" section is a little messy (maybe crevassed), but that the slopes in the cloudy section are really big.  We can use this to define a new filtering strategy that can delete some of the worst segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_difference_filter(D6, tol=2):\n",
    "    \"\"\"\n",
    "    seg_difference_filter: Use elevations and slopes to find bad ATL06 segments\n",
    "    \n",
    "    \n",
    "    Inputs: \n",
    "        D6: a granule of ATL06 data, in dictionary format.  Must have entries:\n",
    "            x_atc, h_li, dh_fit_dx\n",
    "        tol: a tolerance.  Segments whose ends are different from their neighbors \n",
    "             by more than tol are marked as bad\n",
    "    Returns:\n",
    "        good: an array the same size as D6['h_li'].  True  entries indicate that \n",
    "            both ends of the segment are compatible with the segment's neighbors\n",
    "        delta_h_seg: an array the same size as D6['h_li'].  Gives the largest \n",
    "            endpoint difference for each segment    \n",
    "    \"\"\"\n",
    "    h_ep=np.zeros([2, D6['h_li'].size])+np.NaN\n",
    "    h_ep[0, :]=D6['h_li']-D6['dh_fit_dx']*20\n",
    "    h_ep[1, :]=D6['h_li']+D6['dh_fit_dx']*20\n",
    "    delta_h_seg=np.zeros_like(D6['h_li'])\n",
    "    delta_h_seg[1:]=np.abs(D6['h_li'][1:]-h_ep[1, :-1])\n",
    "    delta_h_seg[:-1]=np.maximum(delta_h_seg[:-1], D6['h_li'][:-1]-h_ep[0, 1:])\n",
    "    good=delta_h_seg < tol\n",
    "    return good, delta_h_seg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant Exercise: \n",
    "Try out the seg_difference_filter.  Generate a finalized map of elevations for Byrd Glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to try this, we'll need to add dh_fit_dx to the data dictionary:\n",
    "dataset_dict['land_ice_segments/fit_statistics']=['dh_fit_dx']\n",
    "\n",
    "D6=ATL06_to_dict(files[example_file_indices[0]], dataset_dict)\n",
    "markers=['.','+']\n",
    "plt.figure()\n",
    "for ind, seg in enumerate(D6[0:1]):  \n",
    "    good, delta_h = seg_difference_filter(seg, tol=2)\n",
    "    seg_slope_plot(seg)\n",
    "    plt.plot(seg['x_atc'][good], seg['h_li'][good],'.')\n",
    "plt.title(seg['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of this part of the tutorial:  \n",
    "- Clouds tend to produce weaker-than-normal returns\n",
    "- We can identify low-quality surface returns with a variety of parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Errors due to forward scattering of photons.\n",
    "\n",
    "A more insidious problem that clouds can cause comes about when an optically thin cloud lets photons through, but some of the photons scatter off ice grains in the forward (downward) direction and hit the surface before returning to the telescope.\n",
    "\n",
    "<img src=\"images/Forward_scattering_sm.jpg\"  width=600 height=600>\n",
    "\n",
    "The path for a photon that reaches the surface without hitting the cloud is shown in blue, the path of a scattered photon is shown in red.  Since photons are delayed by a small but random amount whose distribution depends on the height and thickness of the cloud, the expected probability distibution of the scattered photon heights is skewed low (shown in schematic at left), and the mean (or median) photon height has a negative bias. \n",
    "\n",
    "When clouds are high above the ground, most scattered photons end up outside the telescope field of view, and don't do much to skew the return distribution.  It's when clouds are low that more scattered photons are detected, and the bias can become large.  For this reason, Yeukui Yang has put together a set of parameters aimed at flagging segments most strongly affected by forward scattering.  These are in the ATL06 _/gtxx/land_ice_segments/geophysical_ group:\n",
    "\n",
    "There are two cloud parameters:\n",
    "- cloud_flg_asr  : an estimate of the presence/ absence of clouds based on the apparent surface reflectance (low reflectance -> possible clouds)\n",
    "- cloud_flg_atm  : An estimate of the presence / absence of clouds based on backscatter above the surface.  This parameter only gives useful values at night, because during the daytime there is too much solar background to see clouds\n",
    "\n",
    "There are three blowing-snow parameters:\n",
    "- bsnow_h : An estimate of the height of the top of any blowing-snow layers present\n",
    "- bsnow od : An estimate of the optical depth of a blowing-snow layer (if present )\n",
    "- bsnow_conf : A confidence flag for the presence / absence of a blowing-snow layer.\n",
    "\n",
    "And there is a combined flag that summarizes what has been learned from the atmospheric channels:\n",
    "- msw_flag: \"Combined flag indicating the risks of severe multiple scattering. The multiple scattering warning flag (ATL09 parameter msw_flag) has values from -1 to 5 where zero means no multiple scattering and 5 the greatest. If no layers were detected, then msw_flag = 0. If blowing snow is detected and its estimated optical depth is greater than or equal to 0.5, then msw_flag = 5. If the blowing snow optical depth is less than 0.5, then msw_flag = 4. If no blowing snow is detected but there are cloud or aerosol layers detected, the msw_flag assumes values of 1 to 3 based on the height of the bottom of the lowest layer: < 1 km, msw_flag = 3; 1-3 km, msw_flag = 2; > 3km, msw_flag = 1. A value of -1 indicates that the signal to noise of the data was too low to reliably ascertain the presence of cloud or blowing snow. We expect values of -1 to occur only during daylight.\"\n",
    "\n",
    "The long block quote here comes from the product description in ATL06.\n",
    "\n",
    "The algorithms behind these parameters still being refined by the atmosphere team, so it's not certain that they produce good results.  As an exercise for the group, let's split into groups, and map the values for these parameters to see if any of them produce good-looking results over Byrd Glacier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant Exercise: \n",
    "Investigate cloud-filtering parameters over Byrd Glacier.  User the Matplotlib _scatter_ function to map the distribution of the cloud and blowing-snow flags.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## workspace\n",
    "dataset_dict['land_ice_segments/geophysical']=['msw_flag', 'bsnow_od', 'bsnow_conf']\n",
    "\n",
    "D6=ATL06_to_dict(files[example_file_indices[1]], dataset_dict)\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.scatter(D6[1]['x_atc'], D6[1]['h_li'], c=D6[1]['msw_flag'], linewidth=0); plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Directly observing the effects of forward scattering.\n",
    "\n",
    "ATL06 was designed with a scheme to allow us to directly see the effects of forward scattering, or any other process that modifies the shape of the return.  Remember the least-squares fitting scheme?  What do we do with the residuals to the best-fitting line?  Instead of throwing them away, we make a histogram from them, and store it in the _/gtxx/residual_histogram_ group.  Because the histograms for any one segment tend to be a little noisy, we combine groups of ten segments for each \"waveform\" in the group, which gives it an intrinsic horizontal resolution of 200 m.  I've picked out one file where _resurnsidual_histogram_ group shows some returns with scattering and some without.  Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thefile='/home/jovyan/ATL06/Byrd_Glacier_rel001/processed_ATL06_20181017061148_02840111_001_01.h5'\n",
    "\n",
    "rh=ATL06_to_dict(thefile,{'/residual_histogram':['count','bckgrd_per_bin', 'dh','x_atc_mean'],\n",
    "                                    '/land_ice_segments':['h_li','atl06_quality_summary'],\n",
    "                                   '/land_ice_segments/ground_track':['x_atc']})\n",
    "# pick gt1r from the residual histogram:\n",
    "rh=rh[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field we want in the residual histogram group is called _count_.  The along-track centers for each bin in the variable are in _x_atc_mean_, and the relative heights of the bins are in _dh_.  We'll use matplotlib's _imshow_ function to visualize the variable.  I've pre-selected a few profiles that are interesting: One early in the profile (column 25), two later on where the surface starts looking more affected by clouds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "slices=[25, 170, 350]\n",
    "\n",
    "plt.figure(); \n",
    "plt.imshow(rh['count'].T, extent=[rh['x_atc_mean'][0], rh['x_atc_mean'][-1], rh['dh'][0], rh['dh'][-1]], aspect='auto', origin='upper')\n",
    "yl=plt.gca().get_ylim()\n",
    "for slice in slices:\n",
    "    plt.plot(np.zeros(2)+rh['x_atc_mean'][slice], yl,'w:')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there's a sharp return at the left-hand side of the plot, but the returns farther to the right are increasingly dim and spread out.  Let's plot profiles for the three columns I selected and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for this_slice in slices:\n",
    "    plt.plot(rh['count'][this_slice,:], rh['dh'], label=\"column %d\" % this_slice)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly column 25 is a lot sharper than the others, but given the statistical noise and variable amplitude in the profiles, it's hard to see what's going on.  Each profile is binned at 1 cm (check the spacing on the dh variable), which is finer than we need for this plot.  Let's try smoothing the profile a bit to make it nicer to look at, and normalizing each profile to its maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tK=np.arange(-20, 20)*0.01\n",
    "kernel=np.exp(-(tK**2)/2/(0.05**2))\n",
    "kernel=kernel/kernel.sum()\n",
    "kernel.shape=[1, kernel.size]\n",
    "Ps=scipy.signal.convolve2d(rh['count'], kernel,'same')\n",
    "Ps=scipy.signal.convolve2d(Ps, np.ones([5,1])/5,'same')\n",
    "plt.figure()\n",
    "for this_slice in slices:\n",
    "    plt.plot(Ps[this_slice,:]/Ps[this_slice,:].max(), rh['dh'], label=\"column %d\" % this_slice)\n",
    "plt.legend()\n",
    "                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better.  The results are still a little messy, but we can see that column 25 has a pretty sharp peak, column 350 has a sharp peak with a significantly skewed tail on the back side, and column 170 is broad on both sides of zero. Looking at the height profile makes it fairly clear what's happening with column 170:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good=rh['atl06_quality_summary']==0\n",
    "plt.figure()\n",
    "plt.plot(rh['x_atc'][good], rh['h_li'][good],'.')\n",
    "yl=plt.gca().get_ylim()\n",
    "for slice in slices:\n",
    "    plt.plot(np.ones(2)*rh['x_atc_mean'][slice], yl,'k:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like column 170 is broad because of a large surface slope, but the other two profiles are from fairly flat ice.  The shape of the profie at column 350 looks like exactly what we would expect from a return due to forward scattering.  As of yet, nobody (to my knowledge) has tried to relate the cloud parameters to the shape of the surface return.  If there's time, take a minute or two to look at the cloud-clearing parameters for this granule.  If you have time to do this after the tutorial, let me know what you see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
